#!/bin/bash
###############################################################################
# Copyright (C) 2020-2021 Habana Labs, Ltd. an Intel Company
###############################################################################

export ENABLE_EXPERIMENTAL_FLAGS=true # TODO: remove after SW-24230 is done
export OPTIMIZE_DMA_ENGINES_ALLOCATION=1 # TODO: remove after SW-24230 is done

readonly DEFAULT_NUM_WORKERS_PER_HLS=8
readonly SCRIPT_DIR=$( dirname ${BASH_SOURCE[0]} )
readonly COMMON_DIR="$( cd ${SCRIPT_DIR}/../../common && pwd)"
export PYTHONPATH=$PYTHONPATH:$COMMON_DIR

function sub_help()
{
        echo -e "usage: demo_bert subcommand [global and subcommand's local arguments]"
        echo -e "\nsubcommands:"
        echo -e "  help"
        echo -e "  pretraining"
        echo -e "\nglobal mandatory arguments:"
        echo -e "  -d <data_type>,  --dtype <data_type>       Data type, possible values: fp32, bf16"
        echo -e "\nglobal optional arguments:"
        echo -e "                --bert-config-dir            Path to directory containing bert config files needed for choosen training type."
        echo -e "                                             If not specified the zip file will be downloaded"
        echo -e "  -o <dir>,     --output-dir <dir>           Output directory (estimators model_dir)"
        echo -e "  -v [num_workers_per_hls] --use_horovod [num_w]"
        echo -e "                                             Use Horovod for training. num_workers_per_hls parameter is optional and defaults to $DEFAULT_NUM_WORKERS_PER_HLS"
        echo -e "  -hls <hls_type>,  --hls_type <hls_type>    HLS Type, possible values: HLS1, HLS1-H. Default: HLS1"
        echo -e "\npretraining optional arguments:"
        echo -e "  -fpo,               --fast-perf-only                    Run smaller global batch size for perf measurement."
        echo -e "  -evalonly,          --eval_only                         Enable evaluation"
        echo -e "  -dist_eval,         --is_dist_eval_enabled              Enable distributed evaluation"
        echo -e "  -i <val>, --iters <val>                                 Number of steps per worker for each phase of pretraining."
        echo -e "                                                          Default: 8103 for wiki2020"
        echo -e "  -w <val>, --warmup <val>                                Number of warmup steps for each phase of pretraining."
        echo -e "                                                          Default: 0 for wiki2020"
        echo -e "  -b <val>, --batch_size <val>                            Batch size for each phase of pretraining."
        echo -e "                                                          Default: 14 for wiki2020"
        echo -e "  -s <val>, --max_seq_length <val>                        Number of tokens in each sequence for each phase of pretraining."
        echo -e "                                                          Default: 512 for wiki2020"
        echo -e "            --save_ckpt_steps <val>                       How often to save the model checkpoint. Default: 100"
        echo -e "\nexample:"
        echo -e "  demo_bert pretraining -d bf16 -i 30"
        echo -e ""
}

function get_options_pretraining()
{
    while [ -n "$1" ];
        do
            case $1 in
            -i  | --iters )
                __p2_iters=$2
                shift
                ;;
            -w  | --warmup )
                __p2_warmup=$2
                shift
                ;;
            -b  | --batch_size )
                __p2_batch=$2
                shift
                echo "2: "$2
                ;;
            -s  | --max_seq_length )
                __p2_seq_len=$2
                shift
                echo "2: "$2
                ;;
            -fpo | --fast-perf-only )
                __fast_perf_only=1
                shift
                ;;
            --p2_global_batch_size )
                __p2_global_batch_size=$2
                shift
                ;;
            --p2_block_size )
                __p2_block_size=$2
                shift
                ;;
            --learning_rate_phase2 )
                __learning_rate_phase2=$2
                shift
                ;;
            --beta1_phase2 )
                __beta1_phase2=$2
                shift
                ;;
            --beta2_phase2 )
                __beta2_phase2=$2
                shift
                ;;
            --save_ckpt_steps )
                __save_ckpt_steps=$2
                shift
                ;;
            -evalonly | --eval_only )
                __eval_only=1
                ;;
            -dist_eval | --is_dist_eval_enabled )
                __is_dist_eval_enabled=1
                ;;
            -light_ckpt | --use_lightweight_ckpt )
                __use_lightweight_ckpt=1
                ;;
            *)
                sub_help
                print_error "The parameter $1 is not allowed"
                exit 1;
                ;;
            esac
            shift
        done
}

function sub_pretraining()
{
    get_options_pretraining $@
    P2_STEPS=${__p2_iters:=8103}
    P2_WARMUP=${__p2_warmup:=0}
    P2_BATCH=${__p2_batch:=14}
    P2_GLOBAL_BATCH=${__p2_global_batch_size:=448}
    P2_BLOCK_SIZE=${__p2_block_size:=150080}
    LEARNING_RATE_PHASE2=${__learning_rate_phase2:=4e-4}
    BETA1_PHASE2=${__beta1_phase2:=0.9}
    BETA2_PHASE2=${__beta2_phase2:=0.999}
    P2_MAX_SEQ_LENGTH=${__p2_seq_len:=512}
    FAST_PERF_ONLY=${__fast_perf_only:=0}
    USE_DIS_EVAL=${__is_dist_eval_enabled:=0}
    EVAL_ONLY=${__eval_only:=0}
    SAVE_CKPT_STEPS=${__save_ckpt_steps:=100}
    USE_LIGHTWEIGHT_CKPT=${__use_lightweight_ckpt:=0}
    echo -e "EVAL_ONLY=$EVAL_ONLY, USE_DIS_EVAL=$USE_DIS_EVAL"
    echo -e "running MLperf Wiki on BERT ${PRETRAINED_MODEL}; no P1 phase;" \
            "p2: $P2_STEPS steps, $P2_BATCH batch, $P2_GLOBAL_BATCH global batch " \
            "$P2_BLOCK_SIZE block size $P2_MAX_SEQ_LENGTH max_seq_length; " \
            "BF16: ${TF_BF16_CONVERSION:=0}"
    source ${SCRIPT_DIR}/scripts/bert_pretraining_wiki2020_common.sh

}

source $SCRIPT_DIR/../../common/common.sh

# Prevent the user from invoking this script from within mpirun command.
if [ -n "$OMPI_COMM_WORLD_SIZE" ]; then
    print_error "This script is not meant to be ran from within an OpenMPI context (using mpirun). Use -v parameter to enable the multi-node mode."
    exit 1
fi

__use_horovod="false"
__hls_type="HLS1"
__rest_options=() # put unknown options here for subcommand's option parsing

while [ -n "$1" ];
    do
        case $1 in
        -d  | --dtype )
            shift
            __data_type=$1
            ;;
        --bert-config-dir )
            __bert_config_dir=$(realpath -s $2)     # avoid trailing slashes
            shift
            ;;
        -o  | --output-dir )
            shift
            __output_dir=$1
            ;;
        -v  | --use_horovod )
            __use_horovod="true"
            if [ "$2" -eq "$2" ] 2>/dev/null; then  # A fancy way of checking if $2 string an integer.
                export NUM_WORKERS_PER_HLS=$2
                shift
            fi
            ;;
        -hls | --hls_type )
            shift
            __hls_type=$1
            ;;
        -h  | --help )
            sub_help
            exit 1;
            ;;
        *)
            __rest_options+=("$1")
            ;;
        esac
        shift
    done

set -- "${__rest_options[@]}"

if [ -z $__data_type ] ; then
    sub_help
    print_error "-d option must be specified"
    exit 1;
fi

if [ $__data_type == "bf16" ]; then
    export TF_BF16_CONVERSION=${COMMON_DIR}/bf16_config/bert.json
elif [ $__data_type != "fp32" ]; then
    sub_help
    print_error "Incorrect data type passed to -d option"
    exit 1;
fi

# Determine variables specific to the multi-node execution with Horovod enabled.
#
export USE_HOROVOD=$__use_horovod
echo USE_HOROVOD=${USE_HOROVOD}

if [ "$USE_HOROVOD" == "true" ]; then
    __tmp_dir="${HOME}/tmp/"
    run_per_ip mkdir -p "$__tmp_dir"

    # Deprecate NUM_WORKERS.
    if [ -n "$NUM_WORKERS" ]; then
        print_error "NUM_WORKERS environment variable is deprecated in BERT script. Use NUM_WORKERS_PER_HLS."
        exit 128
    fi

    # Numer of processes working in parallel.
    export NUM_WORKERS_PER_HLS=${NUM_WORKERS_PER_HLS:-$DEFAULT_NUM_WORKERS_PER_HLS}
    echo NUM_WORKERS_PER_HLS=$NUM_WORKERS_PER_HLS

    # List of IPs for multi-HLS mode.
    echo MULTI_HLS_IPS=$MULTI_HLS_IPS

    calc_optimal_cpu_resources_for_mpi

    __mpirun_cmd="mpirun"
    __mpirun_cmd+=" --allow-run-as-root"
    __mpirun_cmd+=" --tag-output --merge-stderr-to-stdout --output-filename ${__tmp_dir}/demo_bert_log/"
    __mpirun_cmd+=${MPIRUN_ARGS_MAP_BY_PE}

    if [ -n "$MULTI_HLS_IPS" ]; then
        #
        # Multi-HLS Mode
        #

        export MPI_TPC_INCLUDE=${MPI_TPC_INCLUDE:-enp3s0}
        echo MPI_TPC_INCLUDE=$MPI_TPC_INCLUDE

        # Create HCL config on each remote IP.
        run_per_ip generate_hcl_config '${HOME}/tmp/' ${NUM_WORKERS_PER_HLS} ${__hls_type} &> /dev/null

        # Set HCL_CONFIG_PATH in this script, so it can be propagated in MPIRUN_CMD to remote IPs.
        generate_hcl_config "$__tmp_dir" ${NUM_WORKERS_PER_HLS} ${__hls_type} > /dev/null

        IFS=',' read -ra IPS <<< "$MULTI_HLS_IPS"
        let MPI_NP=${#IPS[@]}*${NUM_WORKERS_PER_HLS}
        export NUM_WORKERS_TOTAL=$MPI_NP
        echo NUM_WORKERS_TOTAL=$NUM_WORKERS_TOTAL

        generate_mpi_hostfile "$__tmp_dir" ${NUM_WORKERS_PER_HLS} > /dev/null
        echo -e "MPI_HOSTFILE_PATH=$MPI_HOSTFILE_PATH ->\n`cat $MPI_HOSTFILE_PATH`"

        __mpirun_cmd+=" -np $NUM_WORKERS_TOTAL"
        __mpirun_cmd+=" --mca plm_rsh_args -p3022"
        __mpirun_cmd+=" --mca btl_tcp_if_include ${MPI_TPC_INCLUDE}"
        __mpirun_cmd+=" -hostfile ${MPI_HOSTFILE_PATH}"
        __mpirun_cmd+=" --prefix /usr/local/openmpi/"  # <- in case you deployed a docker image
        __mpirun_cmd+=" -x HCL_CONFIG_PATH"

        __mpirun_cmd+=" -x HABANA_USE_PREALLOC_BUFFER_FOR_ALLREDUCE"
        __mpirun_cmd+=" -x TF_BF16_CONVERSION"
        __mpirun_cmd+=" -x TF_ALLOW_CONTROL_EDGES_IN_HABANA_OPS"
        __mpirun_cmd+=" -x HBN_TF_REGISTER_DATASETOPS"
        __mpirun_cmd+=" -x HABANA_USE_STREAMS_FOR_HCL"
        __mpirun_cmd+=" -x TF_PRELIMINARY_CLUSTER_SIZE"
        __mpirun_cmd+=" -x HABANA_INITIAL_WORKSPACE_SIZE_MB"
        __mpirun_cmd+=" -x RUN_TPC_FUSER"
        __mpirun_cmd+=" -x TF_DISABLE_SCOPED_ALLOCATOR"
        __mpirun_cmd+=" -x TF_RECIPE_CACHE_PATH"

        __mpirun_cmd+=" -x LD_PRELOAD"
        __mpirun_cmd+=" -x TF_MODULES_RELEASE_BUILD"
        __mpirun_cmd+=" -x PYTHONPATH"
        __mpirun_cmd+=" -x GC_KERNEL_PATH"
        __mpirun_cmd+=" -x HABANA_LOGS"
        __mpirun_cmd+=" -x VIRTUAL_ENV"
        __mpirun_cmd+=" -x PATH"
        __mpirun_cmd+=" -x LD_LIBRARY_PATH"
        __mpirun_cmd+=" -x TF_HCCL_MEMORY_ALLOWANCE_MB"
    else
        #
        # Single-HLS Mode
        #
        export NUM_WORKERS_TOTAL=$NUM_WORKERS_PER_HLS
        echo NUM_WORKERS_TOTAL=$NUM_WORKERS_TOTAL

        generate_hcl_config "$__tmp_dir" ${NUM_WORKERS_PER_HLS} ${__hls_type} > /dev/null
        __mpirun_cmd+=" -np $NUM_WORKERS_PER_HLS"
    fi

    echo -e "HCL_CONFIG_PATH=$HCL_CONFIG_PATH ->\n`cat $HCL_CONFIG_PATH`"

    export MPIRUN_CMD=${MPIRUN_CMD:-$__mpirun_cmd}
    echo MPIRUN_CMD=$MPIRUN_CMD
else
    export NUM_WORKERS_PER_HLS=1
    export NUM_WORKERS_TOTAL=1
    export MPI_MAP_BY=
    export MPI_MAP_BY_PE=
    export MPIRUN_CMD=

    if [ -n "$MULTI_HLS_IPS" ]; then
        echo "warning: In non-Horovod scenario, variable MULTI_HLS_IPS=='$MULTI_HLS_IPS' has no effect."
    fi
fi

if [ -n "$RUN_TPC_FUSER" ]; then echo RUN_TPC_FUSER=${RUN_TPC_FUSER}; fi
if [ -n "$HABANA_SYNAPSE_LOGGER" ]; then echo HABANA_SYNAPSE_LOGGER=${HABANA_SYNAPSE_LOGGER}; fi

PRETRAINED_URL="https://storage.googleapis.com/bert_models/2020_02_20/"


#__model == "large"
PRETRAINED_MODEL="wwm_uncased_L-24_H-1024_A-16"
PRETRAINED_MODEL_NAME="wwm_uncased_L-24_H-1024_A-16"
PRETRAINED_URL="https://storage.googleapis.com/bert_models/2019_05_30/"
# large model is zipped with subdirectory, -j is flattening archive tree structure
EXTRA_UNZIP_PARAM="-j"


if [ -n "$__bert_config_dir" ]; then
    PRETRAINED_MODEL=$__bert_config_dir
fi

if [ -n "$__output_dir" ]; then
    export OUTPUT_DIR=$__output_dir
fi

function download_pretrained_model()
{
    if [[ ! -d "${PRETRAINED_MODEL}" ]]; then
        _wget=false
        if [[ ! -f "${PRETRAINED_MODEL}.zip" ]]; then
            _wget=true
        else
            _check_size=$(wc -c < ${PRETRAINED_MODEL}.zip)
            echo $_check_size

            if [[ $_check_size  -eq 0 ]]; then
                printf "*** Broken file, needs download ...\n\n"
                _wget=true
            fi
        fi

        if [[ "$_wget" == "true" ]]; then
            printf "*** Downloading pre-trained model...\n\n"
            wget ${PRETRAINED_URL}${PRETRAINED_MODEL_NAME}.zip -O ${PRETRAINED_MODEL_NAME}.zip
        fi

        printf "*** Extracting pre-trained model...\n\n"
        unzip ${EXTRA_UNZIP_PARAM} ${PRETRAINED_MODEL_NAME}.zip -d ${PRETRAINED_MODEL}

        if [[ "$_wget" == "true" ]]; then
            rm -rf ${PRETRAINED_MODEL_NAME}.zip
        fi
    fi
}

run_per_ip download_pretrained_model

subcommand=$1
exit_code=0
case $subcommand in
    "" | "-h" | "--help")
        sub_help
       exit 0;
       ;;
    *)
        shift
        sub_${subcommand} $@
        exit_code=$?
        if [ $exit_code = 127 ]; then
            sub_help
            print_error "'$subcommand' is not a known subcommand."
            exit 1
        fi
        ;;
esac
exit $exit_code
