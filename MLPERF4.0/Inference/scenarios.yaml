benchmarks:
  llama2-70b-99.9:
    "rouge1": 43.83612
    "rouge2": 21.6890892
    "rougeL": 28.2219498
  stable-diffusion-xl:
    "FID_SCORE": 23.01085758
    "CLIP_SCORE": 31.68631873
    "FID_SCORE_MAX": 23.95007626
    "CLIP_SCORE_MAX": 31.81331801
scenarios:
  llama-99.9-fp8:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py
    init_setup: ./setup_tgi.sh
    init_Offline: ./run_tgi_server.sh --bs 1024 --scenario Offline --fp8 --output_dir
    init_Server: ./run_tgi_server.sh --bs 768 --scenario Server --fp8 --output_dir
    precision: fp8
    batch_size_Offline: 1024
    batch_size_Server: 768
  sd-xl-fp8:
    dataset: coco-2014
    code_dir: stable-diffusion-xl
    benchmark: stable-diffusion-xl
    command: PT_HPU_GPU_MIGRATION=1 QUANT_CONFIG=tools/quantize/quant_config.json QUANT_CONFIG_2=tools/quantize/quant_config_bmm.json python3 main.py --dataset "coco-1024" --dataset-path coco2014 --profile stable-diffusion-xl-pytorch --dtype bf16 --output build/logs --user_conf configs/user.conf --hpus 8 --max-batchsize 4 --quantize --model-path /mnt/weka/data/mlperf_inference/stable-diffusion-xl/stable_diffusion_fp32/
    precision: fp8
    batch_size: 4
    init_setup: cd tools; ./download-coco-2014.sh -n 1; cd -;
