# Copyright (C) 2023 Habana Labs, Ltd. an Intel Company
#!/usr/bin/env python3
"""Train the HuBERTPretrainModel by using labels generated by KMeans clustering.
Example:
python train.py --dataset-path ./exp/data/mfcc/ --feature-type mfcc --num-classes 100
"""

import os
import logging
import pathlib
import torch, time
from argparse import (ArgumentDefaultsHelpFormatter, ArgumentParser,
                      RawDescriptionHelpFormatter)
from typing import Tuple

from lightning import HuBERTPreTrainModule
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, Callback
from pytorch_lightning.utilities.seed import seed_everything
from pytorch_lightning.utilities.rank_zero import rank_zero_only
from habana_hmp import HPUHMPlugin

logger = logging.getLogger(__name__)


class _Formatter(ArgumentDefaultsHelpFormatter, RawDescriptionHelpFormatter):
    # To use ArgumentDefaultsHelpFormatter as the formatter_class and
    # RawDescriptionHelpFormatter to add custom formatting to description or epilog.
    # Check: https://stackoverflow.com/a/18462760
    pass

class TrainStatCallback(Callback):
    def __init__(self, skip_steps=2):
        self.train_start_time = 0
        self.epoch_start_time = 0
        self.epoch_end_time = 0
        self.train_time = 0
        self.skip_steps = skip_steps
        self.epoch = 0
        self.batch_start_time = 0
        self.val_epoch_start_time = 0
        self.val_epoch_end_time = 0

    def on_train_start(self, _, pl_module):
        self.train_start_time = time.time()

    def on_train_end(self, _, pl_module):
        ttt = time.time() - self.train_start_time
        print("\nTotal Train Time:", ttt, " Total Steps:", _.global_step, " Skipped Steps:", self.skip_steps)

    def on_train_epoch_start(self, _, pl_module):
        self.epoch = _.current_epoch
        print("\n On Train Epoch Start:", self.epoch)
        self.padded_per_epoch = 0
        self.unpadded_per_epoch = 0
        self.epoch_start_time = time.time()

    def on_train_epoch_end(self, _, pl_module):
        epoch_train_time = self.epoch_end_time - self.epoch_start_time
        self.train_time += epoch_train_time
        print("\nEpoch ", self.epoch, " Train Time(s):", epoch_train_time, " Val Time(s)", self.val_epoch_end_time-self.val_epoch_start_time, \
              " Epoch Time(s):", time.time()-self.epoch_start_time)

    def on_train_batch_start(self, _, pl_module, batch, batch_idx):
        if self.epoch == 0 and _.global_step == self.skip_steps:
            self.epoch_start_time = time.time()
        self.batch_start_time = time.time()

    def on_train_batch_end(self, _, pl_module, outputs, batch, batch_idx):
        self.epoch_end_time = time.time()
        #print("\nStep  Time: ", self.epoch_end_time - self.batch_start_time, " for Step:", batch_idx, batch[0].shape, batch[2])

    def on_validation_epoch_start(self, _, pl_module):
        #print("\n On Val Epoch Start:", self.epoch)
        self.val_epoch_start_time = time.time()

    def on_validation_epoch_end(self, _, pl_module):
        self.val_epoch_end_time = time.time()
        #print("\nEpoch Val Time(s):",  time.time()-self.val_epoch_start_time, " for Epoch ", self.epoch)


def run_train(args):
    seed_everything(1337)
    checkpoint_dir = args.exp_dir / f"checkpoints_{args.dataset}_{args.model_name}"
    checkpoint = ModelCheckpoint(
        checkpoint_dir,
        monitor="val_loss",
        mode="min",
        save_top_k=5,
        save_weights_only=False,
        verbose=True,
    )
    train_checkpoint = ModelCheckpoint(
        checkpoint_dir,
        monitor="train_loss",
        mode="min",
        save_top_k=5,
        save_weights_only=False,
        verbose=True,
    )

    callbacks = [
        checkpoint,
        train_checkpoint,
    ]

    plugins = []

    if rank_zero_only.rank == 0:
        train_stat = TrainStatCallback(skip_steps = args.skip_steps)
        callbacks.append(train_stat)

    if args.use_autocast:
        #TODO: DEBUG, it seems autocast only works when this environment variable
        #set with command line itself.
        path = os.path.dirname(os.path.realpath(__file__))
        autocast_bf16 = os.path.join(path, args.autocast_bf16)
        autocast_fp32 = os.path.join(path, args.autocast_fp32)

        os.environ['LOWER_LIST'] = autocast_bf16
        os.environ['FP32_LIST'] = autocast_fp32
        print("AUTOCAST Enabled : ", autocast_bf16, autocast_fp32)

    if args.hmp:
        plugins.append(HPUHMPlugin(verbosity=args.hmp_verbose, model_name='hubert'))

    if args.gpus>-1:
        acc = 'gpu'
        num_devices = args.gpus
        strategy = 'ddp'
    elif args.hpus>-1:
        acc = 'hpu'
        num_devices = args.hpus
        if args.dist_dataset == 0 and num_devices == 1:
            strategy = 'hpu_single'
        else:
            from pytorch_lightning.strategies import HPUParallelStrategy
            strategy = HPUParallelStrategy(broadcast_buffers=False, gradient_as_bucket_view=True, find_unused_parameters=not args.static_layerdrop) # TODO What parameters to pass here?
    else:
        print("Running on CPU ...")
        acc = 'cpu'
        num_devices = 1
        strategy=None


    trainer = Trainer(
        default_root_dir=args.exp_dir,
        max_steps=args.max_updates,
        num_nodes=args.num_nodes,
        accelerator=acc,
        devices=num_devices,
        strategy=strategy,
        replace_sampler_ddp=False,
        plugins = plugins,
        callbacks=callbacks,
        reload_dataloaders_every_n_epochs=1,
        check_val_every_n_epoch=args.check_val_every_n_epoch,
        num_sanity_val_steps=args.num_sanity_val_steps,
        log_every_n_steps=args.log_every_n_steps,
        limit_val_batches=0.0 if os.environ.get("TP_MODEL_PARAM_DUMP_ENABLE","0")=="1" else None, #disable validation for sbs
    )
    model = HuBERTPreTrainModule(
        model_name=args.model_name,
        feature_grad_mult=args.feature_grad_mult,
        num_classes=args.num_classes,
        dataset=args.dataset,
        dataset_path=args.dataset_path,
        feature_type=args.feature_type,
        seconds_per_batch=args.seconds_per_batch,
        learning_rate=args.learning_rate,
        betas=args.betas,
        eps=args.eps,
        weight_decay=args.weight_decay,
        clip_norm=args.clip_norm,
        warmup_updates=args.warmup_updates,
        max_updates=args.max_updates,
        static_logit_generator=args.static_logit_generator,
        static_indexing=args.static_indexing,
        static_layerdrop=args.static_layerdrop,
        align_buckets=args.align_buckets,
        num_buckets=args.num_buckets,
        accum_steps=args.accumulate_grad_batches,
        use_conv2d=args.use_conv2d,
        use_instancenorm=args.use_instancenorm,
        all_deterministic=args.all_deterministic,
        show_traindataloader_stats = args.show_traindataloader_stats,
        no_layerdrop=args.no_layerdrop,
        device=acc, #passing this for sbs, as default self.device in HuBERTPreTrainModule() is cpu
        use_fused_clip=args.use_fused_clip,
        use_max_sub_softmax_opt=args.use_max_sub_softmax_opt,
        recompilation_optimization=args.recompilation_optimization,
        split_logits=args.split_logits,
        optimizer_str=args.optimizer,
        log_every_n_steps=args.log_every_n_steps,
        use_autocast=args.use_autocast,
        save_1d_ckpt=args.save_1d_ckpt
    )
    trainer.fit(model, ckpt_path=args.resume_checkpoint)


def _parse_args():
    parser = ArgumentParser(
        description=__doc__,
        formatter_class=_Formatter,
    )
    parser.add_argument(
        "--dataset-path",
        type=pathlib.Path,
        required=True,
        help="Path to the feature and label directories.",
    )
    parser.add_argument(
        "--resume-checkpoint",
        type=pathlib.Path,
        default=None,
        help="Path to the feature and label directories. (Default: None)",
    )
    parser.add_argument(
        "--feature-type",
        choices=["mfcc", "hubert"],
        type=str,
        required=True,
    )
    parser.add_argument(
        "--feature-grad-mult",
        default=0.1,
        type=float,
        help="The scaling factor to multiply the feature extractor gradient. (Default: 0.1)",
    )
    parser.add_argument(
        "--num-classes",
        choices=[100, 500],
        type=int,
        required=True,
        help="The ``num_class`` when building the hubert_pretrain_base model.",
    )
    parser.add_argument(
        "--model-name",
        default="hubert_pretrain_base",
        choices=["hubert_pretrain_base", "hubert_pretrain_large", "hubert_pretrain_xlarge"],
        type=str,
        help="The HuBERT model to train. (Default: 'hubert_pretrain_base')",
    )
    parser.add_argument(
        "--exp-dir",
        default=pathlib.Path("./exp"),
        type=pathlib.Path,
        help="Directory to save checkpoints and logs to. (Default: './exp')",
    )
    parser.add_argument(
        "--dataset",
        default="librispeech",
        choices=["librispeech", "librilight"],
        type=str,
        help="The dataset for training. (Default: 'librispeech')",
    )
    parser.add_argument(
        "--learning-rate",
        default=0.0005,
        type=float,
        help="The peak learning rate. (Default: 0.0005)",
    )
    parser.add_argument(
        "--betas",
        default=(0.9, 0.98),
        type=Tuple,
        help="The coefficients for computing running averages of gradient and its square (Default: (0.9, 0.98))",
    )
    parser.add_argument(
        "--eps",
        default=1e-6,
        type=float,
        help="Epsilon value in Adam optimizer. (Default: 1e-6)",
    )
    parser.add_argument(
        "--weight-decay",
        default=0.01,
        type=float,
        help="Weight decay (L2 penalty) (default: 0.01)",
    )
    parser.add_argument(
        "--clip-norm",
        default=10.0,
        type=float,
        help="The gradient norm value to clip. (Default: 10.0)",
    )
    parser.add_argument(
        "--num-nodes",
        default=4,
        type=int,
        help="Number of nodes to use for training. (Default: 4)",
    )
    parser.add_argument(
        "--gpus",
        default=-1,
        type=int,
        help="Number of GPUs per node to use for training. (Default: -1)",
    )
    parser.add_argument(
        "--hpus",
        default=-1,
        type=int,
        help="Number of HPUs per node to use for training. (Default: -1)",
    )
    parser.add_argument(
        "--warmup-updates",
        default=32000,
        type=int,
        help="Number of steps for warm up the learning rate. (Default: 32000)",
    )
    parser.add_argument(
        "--max-updates",
        default=250000,
        type=int,
        help="Total number of training steps. (Default: 250000)",
    )
    parser.add_argument(
        "--seconds-per-batch",
        default=87.5,
        type=float,
        help="Number of seconds of audio in a mini-batch. (Default: 87.5)",
    )
    parser.add_argument(
        "--all-static",
        action="store_true",
        help="Use static in all parts of the network where possible. Overrides --static-logit-generator, --static-layerdrop, --static-indexing",
    )
    parser.add_argument(
        "--static-logit-generator",
        action="store_true",
        help="Use static logit genarator",
    )
    parser.add_argument(
        "--static-indexing",
        action="store_true",
        help="Use static indexing",
    )
    parser.add_argument(
        "--static-layerdrop",
        action="store_true",
        help="Use static implementration of layerdrop",
    )
    parser.add_argument(
        "--no-layerdrop",
        action="store_true",
        help="Dont do layer drop at all",
    )
    parser.add_argument(
        "--skip-steps",
        default=1,
        type=int,
        help="Total number of training steps to be skipped for througput calculation. (Default: 1",
    )
    parser.add_argument(
        "--dist-dataset",
        default=1,
        type=int, choices=[0,1],
        help="Use distributed sampler(1) or not(0). (Default: 1",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="whether to use debug level for logging"
    )
    parser.add_argument(
        "--check-val-every-n-epoch",
        default=1,
        type=int,
        help="Perform a validation loop after every N training epochs."
    )
    parser.add_argument(
        "--num-sanity-val-steps",
        default=0,
        type=int,
        help="Sanity check runs n validation batches before starting the training routine."
    )
    # align-buckets can take 3 options:
    # none: leave it as original implementation
    # bucket1: bucket, and use aligned length with sampling (in _update_iter_list)
    # bucket2: bucket, and use unaligned length with sampling (in _update_iter_list)
    #          this mode is intended for unit testing to check parity between none mode and bucketing
    #          note full parity (bit exact loss) is still not expected because of norm layer
    parser.add_argument(
        "--align-buckets",
        default='none',
        choices=['none', 'bucket1', 'bucket2'],
        type=str,
        help="Align sequences to bucket boundaries by padding. Default: none",
    )
    parser.add_argument(
        "--num-buckets",
        default=1000,
        type=int,
        help="Number of buckets (Default: 1000)",
    )
    parser.add_argument(
        "--accumulate-grad-batches",
        default=1,
        type=int,
        help="Gradient accumulation batches for the training."
    )
    parser.add_argument(
        "--use-conv2d",
        action="store_true",
        help="Replace conv1d with conv2d"
    )
    parser.add_argument(
        "--use-instancenorm",
        action="store_true",
        help="Replace groupnorm with instancenorm"
    )

    parser.add_argument(
        "--all-deterministic",
        action="store_true",
        help="Make it all deterministic in all parts of the network where possible. ex> no layer drop, no dropout layer",
    )
    parser.add_argument(
        "--show-traindataloader-stats",
        action="store_true",
        help="Do a dummy run through dataloader and show shape statistics"
    )
    parser.add_argument(
        "--optimizer",
        default="adamw",
        type=str,
        choices = ["adamw", "fusedadamw"],
        help="Optimizer settings"
    )
    parser.add_argument(
        "--use-fused-clip",
        action="store_true",
        help="Use fused clip for performance"
    )
    parser.add_argument(
        "--log-every-n-steps",
        default=300,
        type=int,
        help="Log messages every N training steps."
    )
    parser.add_argument(
        "--use-max-sub-softmax-opt",
        action="store_true",
        help="Use max-sub-softmax optimization for performance"
    )
    parser.add_argument(
        "--recompilation-optimization",
        action="store_true",
        help="Prevent recompiles"
    )
    parser.add_argument(
        "--split-logits",
        action="store_true",
        help="split logits generator"
    )
    parser.add_argument(
        "--save-1d-ckpt",
        action="store_true",
        help="Save feature extractor checkpoint in 1D"
    )


    mixed_precision_group = parser.add_mutually_exclusive_group()
    mixed_precision_group.add_argument('--autocast', dest='use_autocast',action='store_true', help='enable autocast mode on Gaudi')
    mixed_precision_group.add_argument('--hmp', action='store_true', help='enable hmp mode')
    parser.add_argument("--hmp-verbose", action="store_true", help="Display hmp ops")
    parser.add_argument('--autocast-bf16', default='ops_bf16_hubert.txt', help='Path to bf16 ops list in autocast mode')
    parser.add_argument('--autocast-fp32', default='ops_fp32_hubert.txt', help='Path to fp32 ops list in autocast mode')

    args = parser.parse_args()

    if not (args.gpus==-1 and args.hpus==-1):
        assert not (args.gpus!=-1 and args.hpus!=-1)
    if args.all_static:
        args.static_logit_generator = True
        args.static_layerdrop = True
        args.static_indexing = True
    if args.no_layerdrop and args.static_layerdrop:
        print("no_layerdrop and static_layerdrop are both set. no_layerdrop will take effect")
    return args


def _init_logger(debug):
    fmt = "%(asctime)s %(message)s" if debug else "%(message)s"
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(format=fmt, level=level, datefmt="%Y-%m-%d %H:%M:%S")


def cli_main():
    args = _parse_args()
    _init_logger(args.debug)
    run_train(args)


if __name__ == "__main__":
    cli_main()
