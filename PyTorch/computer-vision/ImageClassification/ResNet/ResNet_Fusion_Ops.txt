aten::abs(Tensor self) -> Tensor
aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta = 1, Scalar alpha = 1) ->Tensor
aten::add(Tensor self, Tensor other, *, Scalar alpha) -> Tensor
aten::add(Tensor self, Scalar other, Scalar alpha) -> Tensor
aten::avg_pool2d(Tensor self, int[] kernel_size, int[] stride, int[] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[] kernel_size, int[] stride, int[] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
aten::cat(Tensor[] tensors, int dim) -> Tensor
aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[] output_mask) -> (Tensor, Tensor, Tensor)
aten::div(Tensor self, Tensor other) -> Tensor
aten::div(Tensor self, Scalar other) -> Tensor
aten::eq(Tensor self, Tensor other) -> Tensor
aten::flatten(Tensor self, int start_dim, int end_dim) -> Tensor
aten::gt(Tensor self, Tensor other) -> Tensor
aten::_grad_sum_to_size(Tensor self, int[]? size) -> Tensor
aten::log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
aten::max_pool2d_with_indices(Tensor self, int[] kernel_size, int[] stride, int[] padding, int[] dilation, bool ceil_mode) -> (Tensor, Tensor)
aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[] kernel_size, int[] stride, int[] padding, int[] dilation, bool ceil_mode, Tensor indices) -> Tensor
aten::mm(Tensor self, Tensor mat2) -> Tensor
aten::mul(Tensor self, Scalar other) -> Tensor
aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[] output_mask) -> (Tensor, Tensor, Tensor)
aten::neg(Tensor self) -> Tensor
aten::permute(Tensor self, int[] dims) -> Tensor
aten::relu(Tensor self) -> Tensor
aten::reshape(Tensor self, int[] shape) -> Tensor
aten::select(Tensor self, int dim, int index) -> Tensor
aten::slice(Tensor self, int dim, int start, int end, int step) -> Tensor
aten::sub(Tensor self, Tensor other, *, Scalar alpha) -> Tensor
aten::t(Tensor self) -> Tensor
aten::transpose(Tensor self, int dim0, int dim1) -> Tensor
aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
aten::to(Tensor self, Device device, int dtype, bool non_blocking, bool copy, int? memory_format) -> Tensor
aten::view(Tensor self, int[] size) -> Tensor


