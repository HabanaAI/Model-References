aten::add(Tensor self, Tensor other, *, Scalar alpha) -> Tensor
aten::add(Tensor self, Scalar other, Scalar alpha) -> Tensor
aten::div(Tensor self, Scalar other) -> Tensor
aten::div(Tensor self, Tensor other) -> Tensor
aten::mul(Tensor self, Scalar other) -> Tensor
aten::_grad_sum_to_size(Tensor self, int[]? size) -> Tensor
aten::gt(Tensor self, Tensor other) -> Tensor
aten::eq(Tensor self, Tensor other) -> Tensor
aten::rsub(Tensor self, Scalar other, Scalar alpha) -> Tensor
aten::permute(Tensor self, int[] dims) -> Tensor
aten::flatten(Tensor self, int start_dim, int end_dim) -> Tensor
aten::sub(Tensor self, Tensor other, *, Scalar alpha) -> Tensor
aten::native_layer_norm(Tensor input, Tensor? weight, Tensor? bias, int M, int N, float eps) -> (Tensor, Tensor, Tensor)
aten::native_layer_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int M, int N, bool[] output_mask) -> (Tensor, Tensor, Tensor)
aten::t(Tensor self) -> Tensor
aten::view(Tensor self, int[] size) -> Tensor
aten::transpose(Tensor self, int dim0, int dim1) -> Tensor
aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
aten::log_softmax(Tensor self, int dim, int? dtype) -> Tensor
aten::softmax(Tensor self, int dim, int? dtype) -> Tensor
aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
aten::gelu(Tensor self) -> Tensor
aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
aten::hbgelu2(Tensor self) -> (Tensor, Tensor)
aten::hbgelu2_backward(Tensor grad, Tensor self, Tensor result1) -> Tensor
aten::cat(Tensor[] tensors, int dim) -> Tensor
aten::mul(Tensor self, Tensor other) -> Tensor
aten::matmul(Tensor self, Tensor other) -> Tensor
aten::matmul_backward(Tensor grad_out, Tensor self, Tensor other) -> (Tensor, Tensor)
aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
aten::mm(Tensor self, Tensor mat2) -> Tensor
aten::reshape(Tensor self, int[] shape) -> Tensor
//aten::bernoulli_(Tensor self, float p, *, Generator? generator) -> Tensor
aten::embedding(Tensor weight, Tensor indices, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor, Tensor)
aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
aten::slice(Tensor self, int dim, int start, int end, int step) -> Tensor
aten::expand(Tensor self, int[] size, *, bool implicit) -> Tensor
aten::scatter(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
aten::arange(Scalar start, Scalar end, Scalar step, *, Tensor out) -> Tensor
aten::_fused_dropout(Tensor self, float p, Generator? generator) -> (Tensor, Tensor)
aten::to(Tensor self, Device device, int dtype, bool non_blocking, bool copy, int? memory_format) -> Tensor
aten::to(Tensor self, int dtype, bool non_blocking, bool copy, int? memory_format) -> Tensor
aten::clamp(Tensor self, Scalar? min, Scalar? max) -> Tensor
aten::size(Tensor self) -> int[]
aten::tanh(Tensor self) -> Tensor
prim::dtype(Tensor a) -> int
