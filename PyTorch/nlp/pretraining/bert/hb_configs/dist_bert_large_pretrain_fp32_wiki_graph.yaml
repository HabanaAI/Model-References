###############################################################################
# Copyright (C) 2020-2021 Habana Labs, Ltd. an Intel Company
###############################################################################

##
#./demo_bert_dist --sub-command pretraining --data-type fp32 -b 32 4 -w 8 --data-dir /root/pretrain_ds/hdf5_lower_case_1_seq_len_128/wikicorpus_en /root/pretrain_ds/hdf5_lower_case_1_seq_len_512/wikicorpus_en
##

model: "bert"
env_variables:

parameters:
  # Model name, possible values: base, large
  model_name_or_path: large
  # Training sub-command, possible values: finetuning, pretraining
  command: pretraining
  # Task_name, possible values: mrpc, squad, bookswiki
  task_name: bookswiki
  data_type: fp32
  world_size: 8
  store_true:
    - "accumulate_gradients"
    - "all_reduce_post_accumulation"
    - "all_reduce_post_accumulation_fp16"
    - "dist"

  dataset_parameters:
    mrpc:
      data_type_parameters:
        bf16:
          batch_size: 64
        fp32:
          batch_size: 32
      max_seq_length: 128
      learning_rate: 2e-5
      data_dir: "/root/bert/bert_ds/glue_data/MRPC"

    squad:
      data_type_parameters:
        bf16:
          batch_size: 24
        fp32:
          batch_size: 12
      max_seq_length: 384
      learning_rate: 3e-5
      data_dir: "/root/bert/bert_ds/Squad/"

    bookswiki:
      data_type_parameters:
        bf16:
          batch_size:
            - 64
            - 8
        fp32:
          batch_size:
            - 32
            - 4
      data_dir: "/root/pretrain_ds/hdf5_lower_case_1_seq_len_128/wikicorpus_en,/root/pretrain_ds/hdf5_lower_case_1_seq_len_512/wikicorpus_en"

