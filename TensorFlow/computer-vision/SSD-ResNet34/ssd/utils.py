# Copyright 2018 Google. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Utils for SSD train/eval with low level API."""

from absl import flags

import tensorflow as tf

FLAGS = flags.FLAGS


def wrap_computation_in_while_loop(op_fn, n, host_name):
  """Wraps the ops generated by `op_fn` in tf.while_loop."""

  def computation(i):
    ops = op_fn()
    if not isinstance(ops, list):
      ops = [ops]
    with tf.control_dependencies(ops):
      return i + 1

  with tf.device(device_for_host(host_name)):
    return tf.while_loop(
        lambda i: tf.less(i, n),
        computation, [tf.constant(0)],
        parallel_iterations=1)


def device_for_host(host_name):
  return host_name + '/device:CPU:0'


def device_for_tpu_core(host_name, core=0):
  return host_name + '/device:TPU_REPLICATED_CORE:%d' % core


def tpu_ordinal_fn(shard_index_in_host):
  """Return the TPU ordinal associated with a shard."""
  return shard_index_in_host % FLAGS.num_shards_per_host


class InputDimsFlattener(object):
  """"Flatten input_partition_dims for spatial partition."""

  def __init__(self, input_partition_dims):
    self._initialized = False
    self._flattened_input_dims = None

    # This should have been validated in TPUConfig.
    assert len(input_partition_dims) <= 2, 'must have 1 or 2 elements.'
    if len(input_partition_dims) == 2:
      self._feature_dims, self._label_dims = input_partition_dims
    else:
      self._feature_dims = input_partition_dims[0]
      self._label_dims = None

    assert self._feature_dims is not None, ('input_partition_dims[0] must not '
                                            'be None')

  @property
  def flattened_input_dims(self):
    assert self._initialized, 'InputsStructureRecorder is not initialized.'
    return self._flattened_input_dims

  def validate_and_flatten_input_dims(self, features, labels):
    """Flatten input dims with the same order as flattened input tensors."""

    def _extract_key_names(tensor_or_dict):
      if isinstance(tensor_or_dict, dict):
        return sorted(tensor_or_dict.keys())
      return []

    if self._initialized:
      return self._flattened_input_dims

    has_labels = labels is not None
    feature_names = _extract_key_names(features)
    label_names = _extract_key_names(labels)
    feature_dims_names = _extract_key_names(self._feature_dims)
    if feature_dims_names != feature_names:
      raise ValueError('TPUConfig.input_partition_dims[0] mismatched feature'
                       ' keys. Expected {}, got {}'.format(
                           feature_names, feature_dims_names))

    label_dims_names = _extract_key_names(self._label_dims)
    if self._label_dims is not None and label_dims_names != label_names:
      raise ValueError('TPUConfig.input_partition_dims[1] mismatched label'
                       ' keys. Expected {}, got {}'.format(
                           label_names, label_dims_names))

    flattened_input_dims = []
    if feature_dims_names:
      # We need a fixed ordering for matching the tensors in features.
      flattened_input_dims.extend(
          [self._feature_dims[name] for name in feature_dims_names])
    else:
      flattened_input_dims.append(self._feature_dims)

    if label_dims_names:
      # We need a fixed ordering for matching the tensors in labels.
      flattened_input_dims.extend(
          [self._label_dims[name] for name in label_dims_names])
    else:
      if label_names:
        num_tensors_in_label = len(label_names)
      else:
        num_tensors_in_label = int(has_labels)
      # Setting `None` in input_partition_dims[1] will apply `None` to
      # all the tensors in labels, regardless of internal structure.
      flattened_input_dims.extend([self._label_dims] * num_tensors_in_label)

    self._flattened_input_dims = flattened_input_dims
    self._initialized = True
