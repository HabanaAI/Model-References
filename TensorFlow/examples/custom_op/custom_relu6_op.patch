diff --git a/TensorFlow/examples/custom_op/CMakeLists.txt b/TensorFlow/examples/custom_op/CMakeLists.txt
index 999564cbc..450e7c08e 100644
--- a/TensorFlow/examples/custom_op/CMakeLists.txt
+++ b/TensorFlow/examples/custom_op/CMakeLists.txt
@@ -45,7 +45,7 @@ if(DEFINED USE_CUSTOM_KERNEL)
   add_definitions(-DUSE_CUSTOM_KERNEL=${USE_CUSTOM_KERNEL})
 endif()
 
-add_library(hpu_custom_div_op SHARED hpu_custom_div_op.cpp)
+add_library(hpu_custom_relu6_op SHARED hpu_custom_relu6_op.cpp)
 
 # Get Habana-specific information
 run_py3("import habana_frameworks.tensorflow as htf; \
@@ -59,9 +59,9 @@ list(GET HTF_INFORMATION_LIST 1 HTF_COMPILE_FLAGS)
 string(REPLACE "," ";" HTF_COMPILE_FLAGS ${HTF_COMPILE_FLAGS})
 list(GET HTF_INFORMATION_LIST 2 HTF_LINK_FLAGS)
 
-target_include_directories(hpu_custom_div_op PUBLIC ${HTF_INCLUDE_DIR})
-target_compile_options(hpu_custom_div_op PUBLIC ${TF_COMPILE_FALGS} ${HTF_COMPILE_FLAGS})
-target_link_libraries(hpu_custom_div_op PUBLIC ${TF_LINK_FALGS} ${HTF_LINK_FLAGS})
+target_include_directories(hpu_custom_relu6_op PUBLIC ${HTF_INCLUDE_DIR})
+target_compile_options(hpu_custom_relu6_op PUBLIC ${TF_COMPILE_FALGS} ${HTF_COMPILE_FLAGS})
+target_link_libraries(hpu_custom_relu6_op PUBLIC ${TF_LINK_FALGS} ${HTF_LINK_FLAGS})
 add_definitions(-DINCLUDE_FROM_WHEEL=1)
 
 set(TF_VERSION ${TF_VERSION} CACHE STRING "Version of TF")
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6_grad.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6_grad.py
index ec0a552d3..2dba30a3c 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6_grad.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6_grad.py
@@ -1,7 +1,7 @@
 import tensorflow as tf
 from tensorflow.python.framework import ops
-from habana_frameworks.tensorflow import habana_ops
-custom_op_lib = tf.load_op_library("../../../../../../TensorFlow/examples/custom_op/build/lib/libhpu_custom_relu6_op.so")
+import habana_frameworks.tensorflow as htf
+custom_op_lib = htf.load_op_library("../../../../../../TensorFlow/examples/custom_op/build/lib/libhpu_custom_relu6_op.so")
 custom_relu6_grad = custom_op_lib.custom_relu6_grad_op
 
 @ops.RegisterGradient("CustomRelu6Op")
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
index aa3b54abd..a8fb8ce66 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
@@ -28,6 +28,11 @@ from __future__ import print_function
 import tensorflow.compat.v1 as tf
 import tf_slim as slim
 
+import habana_frameworks.tensorflow as htf
+custom_op_lib = htf.load_op_library("../../../../../../TensorFlow/examples/custom_op/build/lib/libhpu_custom_relu6_op.so")
+custom_relu6 = custom_op_lib.custom_relu6_op
+
+import custom_relu6_grad
 
 def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 35x35 resnet block."""
@@ -51,7 +56,7 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
@@ -77,7 +82,7 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
@@ -103,7 +108,7 @@ def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
index aba27f310..7b7a1b70b 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
@@ -33,6 +33,10 @@ import tf_slim as slim
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
+import habana_frameworks.tensorflow as htf
+custom_op_lib = htf.load_op_library("../../../../../../TensorFlow/examples/custom_op/build/lib/libhpu_custom_relu6_op.so")
+custom_relu6 = custom_op_lib.custom_relu6_op
+
 op = lib.op
 
 expand_input = ops.expand_input_by_factor
@@ -45,7 +49,7 @@ V2_DEF = dict(
         # that's why they are here and not in training_scope.
         (slim.batch_norm,): {'center': True, 'scale': True},
         (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
-            'normalizer_fn': slim.batch_norm, 'activation_fn': tf.nn.relu6
+            'normalizer_fn': slim.batch_norm, 'activation_fn': custom_relu6
         },
         (ops.expanded_conv,): {
             'expansion_size': expand_input(6),
@@ -86,7 +90,7 @@ V2_DEF_GROUP_NORM = copy.deepcopy(V2_DEF)
 V2_DEF_GROUP_NORM['defaults'] = {
     (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
         'normalizer_fn': slim.group_norm,  # pylint: disable=C0330
-        'activation_fn': tf.nn.relu6,  # pylint: disable=C0330
+        'activation_fn': custom_relu6,  # pylint: disable=C0330
     },  # pylint: disable=C0330
     (ops.expanded_conv,): {
         'expansion_size': ops.expand_input_by_factor(6),
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
index 2146844a3..2c528f059 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
@@ -225,7 +225,7 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
 
 def _build_aux_head(net, end_points, num_classes, hparams, scope):
   """Auxiliary head used for all models across all datasets."""
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   with tf.variable_scope(scope):
     aux_logits = tf.identity(net)
     with tf.variable_scope('aux_logits'):
@@ -490,7 +490,7 @@ def _build_nasnet_base(images,
   filter_scaling = 1.0
   # true_cell_num accounts for the stem cells
   true_cell_num = 2 if stem_type == 'imagenet' else 0
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   for cell_num in range(hparams.num_cells):
     stride = 1
     if hparams.skip_reduction_layer_input:
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
index 3fe1ceeee..aefc0e833 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
@@ -34,6 +34,7 @@ from __future__ import print_function
 import tensorflow.compat.v1 as tf
 
 import tf_slim as slim
+import os.path
 
 arg_scope = slim.arg_scope
 DATA_FORMAT_NCHW = 'NCHW'
@@ -43,6 +44,9 @@ INVALID = 'null'
 # that the majority of activation values are in the range [-6, 6].
 CLIP_BY_VALUE_CAP = 6
 
+import habana_frameworks.tensorflow as htf
+custom_op_lib = htf.load_op_library("../../../../../../TensorFlow/examples/custom_op/build/lib/libhpu_custom_relu6_op.so")
+custom_relu6 = custom_op_lib.custom_relu6_op
 
 def calc_reduction_layers(num_cells, num_reduction_layers):
   """Figure out what layers should have reductions."""
@@ -185,7 +189,7 @@ def _stacked_separable_conv(net, stride, operation, filter_size,
                             use_bounded_activation):
   """Takes in an operations and parses it to the correct sep operation."""
   num_layers, kernel_size = _operation_to_info(operation)
-  activation_fn = tf.nn.relu6 if use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if use_bounded_activation else tf.nn.relu
   for layer_num in range(num_layers - 1):
     net = activation_fn(net)
     net = slim.separable_conv2d(
@@ -239,7 +243,7 @@ def _pooling(net, stride, operation, use_bounded_activation):
   padding = 'SAME'
   pooling_type, pooling_shape = _operation_to_pooling_info(operation)
   if use_bounded_activation:
-    net = tf.nn.relu6(net)
+    net = custom_relu6(net)
   if pooling_type == 'avg':
     net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=padding)
   elif pooling_type == 'max':
@@ -286,7 +290,7 @@ class NasNetABaseCell(object):
     prev_num_filters = get_channel_dim(prev_layer.shape)
     curr_filter_shape = int(curr_layer.shape[2])
     prev_filter_shape = int(prev_layer.shape[2])
-    activation_fn = tf.nn.relu6 if self._use_bounded_activation else tf.nn.relu
+    activation_fn = custom_relu6 if self._use_bounded_activation else tf.nn.relu
     if curr_filter_shape != prev_filter_shape:
       prev_layer = activation_fn(prev_layer)
       prev_layer = factorized_reduction(
@@ -305,7 +309,7 @@ class NasNetABaseCell(object):
     # Check to be sure prev layer stuff is setup correctly
     prev_layer = self._reduce_prev_layer(prev_layer, net)
 
-    net = tf.nn.relu6(net) if self._use_bounded_activation else tf.nn.relu(net)
+    net = custom_relu6(net) if self._use_bounded_activation else tf.nn.relu(net)
     net = slim.conv2d(net, num_filters, 1, scope='1x1')
     net = slim.batch_norm(net, scope='beginning_bn')
     # num_or_size_splits=1
@@ -350,7 +354,7 @@ class NasNetABaseCell(object):
           with tf.variable_scope('combine'):
             h = h1 + h2
             if self._use_bounded_activation:
-              h = tf.nn.relu6(h)
+              h = custom_relu6(h)
 
           # Add hiddenstate to the list of hiddenstates we can choose from
           net.append(h)
@@ -375,7 +379,7 @@ class NasNetABaseCell(object):
         net = tf.clip_by_value(net, -CLIP_BY_VALUE_CAP, CLIP_BY_VALUE_CAP)
     elif operation in ['none']:
       if self._use_bounded_activation:
-        net = tf.nn.relu6(net)
+        net = custom_relu6(net)
       # Check if a stride is needed, then use a strided 1x1 here
       if stride > 1 or (input_filters != filter_size):
         if not self._use_bounded_activation:
diff --git a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
index 0851bbbb8..b012ee9ff 100644
--- a/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
+++ b/staging/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
@@ -118,7 +118,7 @@ def _build_pnasnet_base(images,
   filter_scaling = 1.0
   # true_cell_num accounts for the stem cells
   true_cell_num = 2
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   for cell_num in range(hparams.num_cells):
     is_reduction = cell_num in reduction_indices
     stride = 2 if is_reduction else 1
