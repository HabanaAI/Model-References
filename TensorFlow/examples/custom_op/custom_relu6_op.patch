diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6.py
new file mode 100644
index 000000000..cc3278c77
--- /dev/null
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/custom_relu6.py
@@ -0,0 +1,41 @@
+###############################################################################
+# Copyright (C) 2022 Habana Labs, Ltd. an Intel Company
+# All Rights Reserved.
+#
+# Unauthorized copying of this file or any element(s) within it, via any medium
+# is strictly prohibited.
+# This file contains Habana Labs, Ltd. proprietary and confidential information
+# and is subject to the confidentiality and license agreements under which it
+# was provided.
+#
+###############################################################################
+import os
+from tensorflow.python.framework import ops
+import habana_frameworks.tensorflow as htf
+import TensorFlow
+
+htf.load_habana_module()
+
+path_TF_module = TensorFlow.__path__[0] # TensorFlow is a namespace module within this repo
+custom_op_lib = htf.load_op_library(
+    os.path.join(path_TF_module, "examples", "custom_op", "build", "lib", "libhpu_custom_relu6_op.so")
+)
+custom_relu6 = custom_op_lib.custom_relu6_op
+custom_relu6_grad = custom_op_lib.custom_relu6_grad_op
+
+_is_grad_registered = False
+
+
+def _register_grad():
+    global _is_grad_registered
+    if _is_grad_registered:
+        return
+
+    @ops.RegisterGradient("CustomRelu6Op")
+    def _CustomRelu6Grad(op, grad):
+        return custom_relu6_grad(grad, op.outputs[0])
+
+    _is_grad_registered = True
+
+
+_register_grad()
diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
index aa3b54abd..b1df1b99b 100644
--- a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/inception_resnet_v2.py
@@ -28,6 +28,7 @@ from __future__ import print_function
 import tensorflow.compat.v1 as tf
 import tf_slim as slim
 
+from custom_relu6 import custom_relu6
 
 def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 35x35 resnet block."""
@@ -51,7 +52,7 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
@@ -77,7 +78,7 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
@@ -103,7 +104,7 @@ def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
     net += scaled_up
     if activation_fn:
-      net = activation_fn(net)
+      net = custom_relu6(net)
   return net
 
 
diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
index aba27f310..2cb6e30a9 100644
--- a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/mobilenet/mobilenet_v2.py
@@ -33,6 +33,8 @@ import tf_slim as slim
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
+from custom_relu6 import custom_relu6
+
 op = lib.op
 
 expand_input = ops.expand_input_by_factor
@@ -45,7 +47,7 @@ V2_DEF = dict(
         # that's why they are here and not in training_scope.
         (slim.batch_norm,): {'center': True, 'scale': True},
         (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
-            'normalizer_fn': slim.batch_norm, 'activation_fn': tf.nn.relu6
+            'normalizer_fn': slim.batch_norm, 'activation_fn': custom_relu6
         },
         (ops.expanded_conv,): {
             'expansion_size': expand_input(6),
@@ -86,7 +88,7 @@ V2_DEF_GROUP_NORM = copy.deepcopy(V2_DEF)
 V2_DEF_GROUP_NORM['defaults'] = {
     (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
         'normalizer_fn': slim.group_norm,  # pylint: disable=C0330
-        'activation_fn': tf.nn.relu6,  # pylint: disable=C0330
+        'activation_fn': custom_relu6,  # pylint: disable=C0330
     },  # pylint: disable=C0330
     (ops.expanded_conv,): {
         'expansion_size': ops.expand_input_by_factor(6),
diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
index 2146844a3..467935a7c 100644
--- a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet.py
@@ -30,6 +30,7 @@ from nets.nasnet import nasnet_utils
 
 arg_scope = slim.arg_scope
 
+from custom_relu6 import custom_relu6
 
 # Notes for training NASNet Cifar Model
 # -------------------------------------
@@ -225,7 +226,7 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
 
 def _build_aux_head(net, end_points, num_classes, hparams, scope):
   """Auxiliary head used for all models across all datasets."""
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   with tf.variable_scope(scope):
     aux_logits = tf.identity(net)
     with tf.variable_scope('aux_logits'):
@@ -490,7 +491,7 @@ def _build_nasnet_base(images,
   filter_scaling = 1.0
   # true_cell_num accounts for the stem cells
   true_cell_num = 2 if stem_type == 'imagenet' else 0
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   for cell_num in range(hparams.num_cells):
     stride = 1
     if hparams.skip_reduction_layer_input:
diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
index 3fe1ceeee..803d39165 100644
--- a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/nasnet_utils.py
@@ -43,6 +43,7 @@ INVALID = 'null'
 # that the majority of activation values are in the range [-6, 6].
 CLIP_BY_VALUE_CAP = 6
 
+from custom_relu6 import custom_relu6
 
 def calc_reduction_layers(num_cells, num_reduction_layers):
   """Figure out what layers should have reductions."""
@@ -185,7 +186,7 @@ def _stacked_separable_conv(net, stride, operation, filter_size,
                             use_bounded_activation):
   """Takes in an operations and parses it to the correct sep operation."""
   num_layers, kernel_size = _operation_to_info(operation)
-  activation_fn = tf.nn.relu6 if use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if use_bounded_activation else tf.nn.relu
   for layer_num in range(num_layers - 1):
     net = activation_fn(net)
     net = slim.separable_conv2d(
@@ -239,7 +240,7 @@ def _pooling(net, stride, operation, use_bounded_activation):
   padding = 'SAME'
   pooling_type, pooling_shape = _operation_to_pooling_info(operation)
   if use_bounded_activation:
-    net = tf.nn.relu6(net)
+    net = custom_relu6(net)
   if pooling_type == 'avg':
     net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=padding)
   elif pooling_type == 'max':
@@ -286,7 +287,7 @@ class NasNetABaseCell(object):
     prev_num_filters = get_channel_dim(prev_layer.shape)
     curr_filter_shape = int(curr_layer.shape[2])
     prev_filter_shape = int(prev_layer.shape[2])
-    activation_fn = tf.nn.relu6 if self._use_bounded_activation else tf.nn.relu
+    activation_fn = custom_relu6 if self._use_bounded_activation else tf.nn.relu
     if curr_filter_shape != prev_filter_shape:
       prev_layer = activation_fn(prev_layer)
       prev_layer = factorized_reduction(
@@ -305,7 +306,7 @@ class NasNetABaseCell(object):
     # Check to be sure prev layer stuff is setup correctly
     prev_layer = self._reduce_prev_layer(prev_layer, net)
 
-    net = tf.nn.relu6(net) if self._use_bounded_activation else tf.nn.relu(net)
+    net = custom_relu6(net) if self._use_bounded_activation else tf.nn.relu(net)
     net = slim.conv2d(net, num_filters, 1, scope='1x1')
     net = slim.batch_norm(net, scope='beginning_bn')
     # num_or_size_splits=1
@@ -350,7 +351,7 @@ class NasNetABaseCell(object):
           with tf.variable_scope('combine'):
             h = h1 + h2
             if self._use_bounded_activation:
-              h = tf.nn.relu6(h)
+              h = custom_relu6(h)
 
           # Add hiddenstate to the list of hiddenstates we can choose from
           net.append(h)
@@ -375,7 +376,7 @@ class NasNetABaseCell(object):
         net = tf.clip_by_value(net, -CLIP_BY_VALUE_CAP, CLIP_BY_VALUE_CAP)
     elif operation in ['none']:
       if self._use_bounded_activation:
-        net = tf.nn.relu6(net)
+        net = custom_relu6(net)
       # Check if a stride is needed, then use a strided 1x1 here
       if stride > 1 or (input_filters != filter_size):
         if not self._use_bounded_activation:
diff --git a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
index 0851bbbb8..214833efb 100644
--- a/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
+++ b/TensorFlow/computer_vision/mobilenetv2/research/slim/nets/nasnet/pnasnet.py
@@ -32,6 +32,7 @@ from nets.nasnet import nasnet_utils
 
 arg_scope = slim.arg_scope
 
+from custom_relu6 import custom_relu6
 
 def large_imagenet_config():
   """Large ImageNet configuration based on PNASNet-5."""
@@ -118,7 +119,7 @@ def _build_pnasnet_base(images,
   filter_scaling = 1.0
   # true_cell_num accounts for the stem cells
   true_cell_num = 2
-  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
+  activation_fn = custom_relu6 if hparams.use_bounded_activation else tf.nn.relu
   for cell_num in range(hparams.num_cells):
     is_reduction = cell_num in reduction_indices
     stride = 2 if is_reduction else 1
diff --git a/TensorFlow/examples/custom_op/CMakeLists.txt b/TensorFlow/examples/custom_op/CMakeLists.txt
index 80b334de8..71be8813a 100644
--- a/TensorFlow/examples/custom_op/CMakeLists.txt
+++ b/TensorFlow/examples/custom_op/CMakeLists.txt
@@ -45,7 +45,7 @@ if(DEFINED USE_CUSTOM_KERNEL)
   add_definitions(-DUSE_CUSTOM_KERNEL=${USE_CUSTOM_KERNEL})
 endif()
 
-add_library(hpu_custom_div_op SHARED hpu_custom_div_op.cpp)
+add_library(hpu_custom_relu6_op SHARED hpu_custom_relu6_op.cpp)
 
 # Get Habana-specific information
 run_py3("import habana_frameworks.tensorflow as htf; \
@@ -59,9 +59,9 @@ list(GET HTF_INFORMATION_LIST 1 HTF_COMPILE_FLAGS)
 string(REPLACE "," ";" HTF_COMPILE_FLAGS ${HTF_COMPILE_FLAGS})
 list(GET HTF_INFORMATION_LIST 2 HTF_LINK_FLAGS)
 
-target_include_directories(hpu_custom_div_op PUBLIC ${HTF_INCLUDE_DIR})
-target_compile_options(hpu_custom_div_op PUBLIC ${TF_COMPILE_FALGS} ${HTF_COMPILE_FLAGS})
-target_link_libraries(hpu_custom_div_op PUBLIC ${TF_LINK_FALGS} ${HTF_LINK_FLAGS})
+target_include_directories(hpu_custom_relu6_op PUBLIC ${HTF_INCLUDE_DIR})
+target_compile_options(hpu_custom_relu6_op PUBLIC ${TF_COMPILE_FALGS} ${HTF_COMPILE_FLAGS})
+target_link_libraries(hpu_custom_relu6_op PUBLIC ${TF_LINK_FALGS} ${HTF_LINK_FLAGS})
 add_definitions(-DINCLUDE_FROM_WHEEL=1)
 
 set(TF_VERSION ${TF_VERSION} CACHE STRING "Version of TF")
@@ -69,7 +69,7 @@ set(TF_COMPILE_FALGS ${TF_COMPILE_FALGS} CACHE STRING "Set of TF compiler flags"
 set(TF_LINK_FALGS ${TF_LINK_FALGS} CACHE STRING "Set of TF linker flags")
 set(TF_FOUND ${TF_FOUND} CACHE BOOL "If false, do not try to use TENSORFLOW")
 if (TF_VERSION MATCHES "^2\.8\.*" OR TF_VERSION MATCHES "^2\.9\.*")
-    set_target_properties(hpu_custom_div_op PROPERTIES CXX_STANDARD 14 CXX_STANDARD_REQUIRED ON)
+    set_target_properties(hpu_custom_relu6_op PROPERTIES CXX_STANDARD 14 CXX_STANDARD_REQUIRED ON)
 else()
-    set_target_properties(hpu_custom_div_op PROPERTIES CXX_STANDARD 17 CXX_STANDARD_REQUIRED ON)
+    set_target_properties(hpu_custom_relu6_op PROPERTIES CXX_STANDARD 17 CXX_STANDARD_REQUIRED ON)
 endif()
