#!/bin/bash

readonly DEFAULT_NUM_WORKERS=8
readonly SCRIPT_DIR=$( dirname ${BASH_SOURCE[0]} )
readonly COMMON_DIR="$( cd ../../common && pwd)"
export PYTHONPATH=$PYTHONPATH:$COMMON_DIR

function sub_help()
{
        echo -e "usage: demo_bert subcommand [global and subcommand's local arguments]"
        echo -e "\nsubcommands:"
        echo -e "  help"
        echo -e "  finetuning"
        echo -e "  pretraining"
        echo -e "\nglobal mandatory arguments:"
        echo -e "  -d <data_type>,  --dtype <data_type>       Data type, possible values: fp32, bf16"
        echo -e "  -m <model>,      --model <model>           Model variant, possible values: tiny, mini, small, medium, base, large"
        echo -e "\nglobal optional arguments:"
        echo -e "  -o <dir>,     --output-dir <dir>           Output directory (estimators model_dir)"
        echo -e "  -v [num_workers] --use_horovod [num_w]     Use Horovod for training. num_workers parameter is optional and defaults to $DEFAULT_NUM_WORKERS"
        echo -e "\nfinetuning mandatory arguments:"
        echo -e "  -t <test_set>,   --test-set <test_set>     Benchmark dataset, possible values: mrpc, squad"
        echo -e "\nfinetuning optional arguments:"
        echo -e "  -e <val>,     --epochs <val>               Number of epochs. If not set defaults to 3.0 for mrpc, 2.0 for squad and 40.0 for bookswiki"
        echo -e "  -b <val>,     --batch_size <val>           Batch size"
        echo -e "  -s <val>,     --max_seq_length <val>       Number of tokens in each sequence"
        echo -e "\npretraining optional arguments:"
        echo -e "  -t <test_set>,      --test-set <test_set>               Benchmark dataset, possible values: bookswiki [default], overfit."
        echo -e "  For below options, 2nd value is valid only for -t bookswiki:"
        echo -e "  -i <val1> [<val2>], --iters <val1> [<val2>]             Number of steps for each phase of pretraining."
        echo -e "                                                          Default: 900000 400000 for bookswiki and 200 for overfit"
        echo -e "  -w <val1> [<val2>], --warmup <val1> [<val2>]            Number of warmup steps for each phase of pretraining."
        echo -e "                                                          Default: 10000 4000 for bookswiki and 10 for overfit"
        echo -e "  -b <val1> [<val2>], --batch_size <val1> [<val2>]        Batch size for each phase of pretraining."
        echo -e "                                                          Default: 32 10 for bookswiki and 32 for overfit"
        echo -e "\nexample:"
        echo -e "  demo_bert finetuning -d bf16 -m base -t mrpc -e 0.5"
        echo -e "  demo_bert pretraining -d fp32 -m large -i 100 30"
        echo -e ""
}


function get_options_finetuning()
{
    while [ -n "$1" ];
        do
            case $1 in
            -t  | --test-set )
                shift
                __test_set=$1
                ;;
            -e  | --epochs )
                shift
                __epochs=$1
                ;;
            -b  | --batch_size )
                shift
                __batch=$1
                ;;
            -s  | --max_seq_length )
                shift
                __seq=$1
                ;;
            *)
                sub_help
                print_error "The parameter $1 is not allowed"
                exit 1;
                ;;
            esac
            shift
        done
}

function sub_finetuning()
{
    get_options_finetuning $@

    if [ -z $__test_set ]; then
        sub_help
        print_error "-t option must be specified"
        exit 1;
    fi

    if [ $__test_set == "mrpc" ]; then
        TRAIN_EPOCHS=${__epochs:=3}
        if [ $__data_type == "bf16" ]; then
            TRAIN_BATCH=${__batch:=64}
        else
            TRAIN_BATCH=${__batch:=32}
        fi
        MAX_SEQ_LENGTH=${__seq:=128}
        echo -e "running MRPC on BERT ${PRETRAINED_MODEL} epochs: ${TRAIN_EPOCHS} BF16: ${TF_ENABLE_BF16_CONVERSION:=0} BS: ${TRAIN_BATCH} T: ${MAX_SEQ_LENGTH}"
        source ${SCRIPT_DIR}/bert_mrpc_common.sh
    elif [ $__test_set == "squad" ]; then
        TRAIN_EPOCHS=${__epochs:=2}
        if [ $__data_type == "bf16" ]; then
            TRAIN_BATCH=${__batch:=24}
        else
            TRAIN_BATCH=${__batch:=10}
        fi
        MAX_SEQ_LENGTH=${__seq:=384}
        echo -e "running SQUAD on BERT ${PRETRAINED_MODEL} epochs: ${TRAIN_EPOCHS} BF16: ${TF_ENABLE_BF16_CONVERSION:=0} BS: ${TRAIN_BATCH} T: ${MAX_SEQ_LENGTH}"
        source ${SCRIPT_DIR}/bert_squad_common.sh
    else
        sub_help
        print_error "Incorrect test set passed to -t option"
    fi
}

function get_options_pretraining()
{
    __test_set=bookswiki
    while [ -n "$1" ];
        do
            case $1 in
            -i  | --iters )
                __p1_iters=$2
                shift
                if [[ $2 =~ ^[0-9]+$ ]]; then __p2_iters=$2; shift; fi
                ;;
            -w  | --warmup )
                __p1_warmup=$2
                shift
                if [[ $2 =~ ^[0-9]+$ ]]; then __p2_warmup=$2; shift; fi
                ;;
            -b  | --batch_size )
                __p1_batch=$2
                shift
                echo "2: "$2
                if [[ $2 =~ ^[0-9]+$ ]]; then __p2_batch=$2; shift; fi
                ;;
            -t  | --test-set )
                __test_set=$2
                shift
                ;;
            *)
                sub_help
                print_error "The parameter $1 is not allowed"
                exit 1;
                ;;
            esac
            shift
        done
}

function sub_pretraining()
{
    get_options_pretraining $@
    if [ $__test_set == "bookswiki" ]; then
        P1_STEPS=${__p1_iters:=900000}
        P2_STEPS=${__p2_iters:=400000}
        P1_WARMUP=${__p1_warmup:=10000}
        P2_WARMUP=${__p2_warmup:=4000}
        if [ $__data_type == "bf16" ]; then
            P1_BATCH=${__p1_batch:=64}
        else
            P1_BATCH=${__p1_batch:=32}
        fi
        P2_BATCH=${__p2_batch:=8}
        echo -e "running Books & Wiki on BERT ${PRETRAINED_MODEL}; p1: $P1_STEPS steps, $P1_BATCH batch; p2: $P2_STEPS steps, $P2_BATCH batch; BF16: ${TF_ENABLE_BF16_CONVERSION:=0}"
        source ${SCRIPT_DIR}/bert_pretraining_bookswiki_common.sh
    elif [ $__test_set == "overfit" ]; then
        P1_STEPS=${__p1_iters:=200}
        P1_WARMUP=${__p1_warmup:=10}
        P1_BATCH=${__p1_batch:=32}
        echo -e "running overfit on BERT ${PRETRAINED_MODEL}; $P1_STEPS steps, $P1_BATCH batch; BF16: ${TF_ENABLE_BF16_CONVERSION:=0} "
        source ${SCRIPT_DIR}/bert_pretraining_overfit_common.sh
    else
        sub_help
        print_error "Incorrect test set passed to -t option. Possible values: bookswiki, overfit"
    fi

}

source $SCRIPT_DIR/../../common/common.sh

# Prevent the user from invoking this script from within mpirun command.
if [ -n "$OMPI_COMM_WORLD_SIZE" ]; then
    print_error "This script is not meant to be ran from within an OpenMPI context (using mpirun). Use -v parameter to enable the multi-node mode."
    exit 1
fi

__use_horovod="false"
__rest_options=() # put unknown options here for subcommand's option parsing

while [ -n "$1" ];
    do
        case $1 in
        -d  | --dtype )
            shift
            __data_type=$1
            ;;
        -m  | --model )
            shift
            __model=$1
            ;;
        -o  | --output-dir )
            shift
            __output_dir=$1
            ;;
        -v  | --use_horovod )
            __use_horovod="true"
            if [ "$2" -eq "$2" ] 2>/dev/null; then  # A fancy way of checking if $2 string an integer.
                export NUM_WORKERS=$2
                shift
            fi
            ;;
        -h  | --help )
            sub_help
            exit 1;
            ;;
        *)
            __rest_options+=("$1")
            ;;
        esac
        shift
    done

set -- "${__rest_options[@]}"

if [ -z $__data_type ] || [ -z $__model ]; then
    sub_help
    print_error "-d and -m options must be specified"
    exit 1;
fi

if [ $__data_type == "bf16" ]; then
    export TF_ENABLE_BF16_CONVERSION=bert
elif [ $__data_type != "fp32" ]; then
    sub_help
    print_error "Incorrect data type passed to -d option"
    exit 1;
fi

# Determine variables specific to the multi-node execution with Horovod enabled.
#
export USE_HOROVOD=$__use_horovod
echo USE_HOROVOD=${USE_HOROVOD}

if [ "$USE_HOROVOD" == "true" ]; then
    __tmp_dir="${HOME}/tmp/"
    mkdir -p "$__tmp_dir"

    # Numer of processes working in parallel.
    export NUM_WORKERS=${NUM_WORKERS:-$DEFAULT_NUM_WORKERS}
    echo NUM_WORKERS=$NUM_WORKERS

    # List of IPs for multi-HLS mode.
    echo MULTI_HLS_IPS=$MULTI_HLS_IPS

    # OpenMPI process bind resource type.
    export MPI_MAP_BY=${MPI_MAP_BY:-"socket"}
    echo MPI_MAP_BY=$MPI_MAP_BY

    # Determine the optimal value of resources per process of OpenMPI binding based on local lscpu.
    if [ "$MPI_MAP_BY" == "socket" ]; then
        __mpi_map_by_pe=`lscpu | grep "CPU(s):" | python3 -c "print(int(input().split()[1])//${NUM_WORKERS}//2)"`
    elif [ "$MPI_MAP_BY" == "slot" ]; then
        __mpi_map_by_pe=`lscpu | grep "CPU(s):" | python3 -c "print(int(input().split()[1])//${NUM_WORKERS})"`
    else
        print_error "MPI_MAP_BY must be either 'socket' or 'slot'."
        exit 1;
    fi
    export MPI_MAP_BY_PE=${MPI_MAP_BY_PE:-$__mpi_map_by_pe}
    echo MPI_MAP_BY_PE=$MPI_MAP_BY_PE

    __mpirun_cmd="mpirun"
    __mpirun_cmd+=" --allow-run-as-root"
    __mpirun_cmd+=" --tag-output --merge-stderr-to-stdout --output-filename ${__tmp_dir}/demo_bert_log/"

    if [ "$MPI_MAP_BY_PE" -gt "0" ]; then
        __mpirun_cmd+=" --bind-to core --map-by $MPI_MAP_BY:PE=$MPI_MAP_BY_PE"
    fi

    if [ -n "$MULTI_HLS_IPS" ]; then
        #
        # Multi-HLS Mode
        #

        # Create HCL config on each remote IP.
        run_per_ip generate_hcl_config '${HOME}/tmp/' ${NUM_WORKERS} &> /dev/null

        # Set HCL_CONFIG_PATH in this script, so it can be propagated in MPIRUN_CMD to remote IPs.
        generate_hcl_config "$__tmp_dir" ${NUM_WORKERS} > /dev/null

        IFS=',' read -ra IPS <<< "$MULTI_HLS_IPS"
        let MPI_NP=${#IPS[@]}*8

        generate_mpi_hostfile "$__tmp_dir" > /dev/null
        echo -e "MPI_HOSTFILE_PATH=$MPI_HOSTFILE_PATH ->\n`cat $MPI_HOSTFILE_PATH`"

        __mpirun_cmd+=" -np $MPI_NP"
        #__mpirun_cmd+=" --mca plm_rsh_args -p3022"  # <- This causes "connection refused" when connecting to igk machines
        __mpirun_cmd+=" --mca btl_tcp_if_exclude lo,docker0"
        __mpirun_cmd+=" -hostfile ${MPI_HOSTFILE_PATH}"
        #__mpirun_cmd+=" --prefix /usr/lib/habanalabs/openmpi/"  # <- in case you deployed a docker image
        __mpirun_cmd+=" --prefix ${HOME}/.openmpi/"  # <- in case you invoked build_horovod manually
        __mpirun_cmd+=" -x HCL_CONFIG_PATH"

        __mpirun_cmd+=" -x HABANA_USE_PREALLOC_BUFFER_FOR_ALLREDUCE"
        __mpirun_cmd+=" -x TF_ENABLE_BF16_CONVERSION"
        __mpirun_cmd+=" -x TF_ALLOW_CONTROL_EDGES_IN_HABANA_OPS"
        __mpirun_cmd+=" -x HBN_TF_REGISTER_DATASETOPS"
        __mpirun_cmd+=" -x HABANA_USE_STREAMS_FOR_HCL"
        __mpirun_cmd+=" -x TF_PRELIMINARY_CLUSTER_SIZE"
        __mpirun_cmd+=" -x HABANA_INITIAL_WORKSPACE_SIZE_MB"

        __mpirun_cmd+=" -x LD_PRELOAD"
        __mpirun_cmd+=" -x TF_MODULES_RELEASE_BUILD"
        __mpirun_cmd+=" -x PYTHONPATH"
        __mpirun_cmd+=" -x GC_KERNEL_PATH"
        __mpirun_cmd+=" -x HABANA_LOGS"
        __mpirun_cmd+=" -x VIRTUAL_ENV"
        __mpirun_cmd+=" -x PATH"
        __mpirun_cmd+=" -x LD_LIBRARY_PATH"
    else
        #
        # Single-HLS Mode
        #
        generate_hcl_config "$__tmp_dir" ${NUM_WORKERS} > /dev/null
        __mpirun_cmd+=" -np $NUM_WORKERS"
    fi

    echo -e "HCL_CONFIG_PATH=$HCL_CONFIG_PATH ->\n`cat $HCL_CONFIG_PATH`"

    export MPIRUN_CMD=${MPIRUN_CMD:-$__mpirun_cmd}
    echo MPIRUN_CMD=$MPIRUN_CMD
else
    export NUM_WORKERS=1
    export MPI_MAP_BY=
    export MPI_MAP_BY_PE=
    export MPIRUN_CMD=

    if [ -n "$MULTI_HLS_IPS" ]; then
        echo "warning: In non-Horovod scenario, variable MULTI_HLS_IPS=='$MULTI_HLS_IPS' has no effect."
    fi
fi

if [ -n "$RUN_TPC_FUSER" ]; then echo RUN_TPC_FUSER=${RUN_TPC_FUSER}; else export RUN_TPC_FUSER=true; fi
if [ -n "$HABANA_SYNAPSE_LOGGER" ]; then echo HABANA_SYNAPSE_LOGGER=${HABANA_SYNAPSE_LOGGER}; fi

PRETRAINED_URL="https://storage.googleapis.com/bert_models/2020_02_20/"
if [ $__model == "base" ]; then
    PRETRAINED_MODEL="uncased_L-12_H-768_A-12"
    # cluster slicing optimization tested for bert base, works well with this size
    export TF_PRELIMINARY_CLUSTER_SIZE=1000
elif [ $__model == "large" ]; then
    PRETRAINED_MODEL="wwm_uncased_L-24_H-1024_A-16"
    PRETRAINED_URL="https://storage.googleapis.com/bert_models/2019_05_30/"
    # large model is zipped with subdirectory, -j is flattening archive tree structure
    EXTRA_UNZIP_PARAM="-j"
elif [ $__model == "tiny" ]; then
    PRETRAINED_MODEL="uncased_L-2_H-128_A-2"
elif [ $__model == "mini" ]; then
    PRETRAINED_MODEL="uncased_L-4_H-256_A-4"
elif [ $__model == "small" ]; then
    PRETRAINED_MODEL="uncased_L-4_H-512_A-8"
elif [ $__model == "medium" ]; then
    PRETRAINED_MODEL="uncased_L-8_H-512_A-8"
else
    sub_help
    print_error "Incorrect model passed to -m option"
    exit 1;
fi

MODEL_TYPE=$__model

if [ -n "$__output_dir" ]; then
    export OUTPUT_DIR=$__output_dir
fi

function download_pretrained_model()
{
    if [[ ! -d "${PRETRAINED_MODEL}" ]]; then
        printf "*** Downloading pre-trained model...\n\n"
        wget ${PRETRAINED_URL}${PRETRAINED_MODEL}.zip -O ${PRETRAINED_MODEL}.zip
        printf "*** Extracting pre-trained model...\n\n"
        unzip ${EXTRA_UNZIP_PARAM} ${PRETRAINED_MODEL}.zip -d ${PRETRAINED_MODEL}
        rm -rf ${PRETRAINED_MODEL}.zip
    fi
}

run_per_ip download_pretrained_model

subcommand=$1
case $subcommand in
    "" | "-h" | "--help")
        sub_help
       exit 0;
       ;;
    *)
        shift
        sub_${subcommand} $@
        if [ $? = 127 ]; then
            sub_help
            print_error "'$subcommand' is not a known subcommand."
            exit 1
        fi
        ;;
esac
