###############################################################################
# Copyright (C) 2020-2021 Habana Labs, Ltd. an Intel Company
# All Rights Reserved.
#
# Unauthorized copying of this file or any element(s) within it, via any medium
# is strictly prohibited.
# This file contains Habana Labs, Ltd. proprietary and confidential information
# and is subject to the confidentiality and license agreements under which it
# was provided.
###############################################################################

model: "bert"
env_variables:
  LOG_LEVEL_ALL: 6
  LD_PRELOAD: "/usr/lib/x86_64-linux-gnu/libjemalloc.so.1"
  # TODO: remove after SW-24230 is done
  ENABLE_EXPERIMENTAL_FLAGS: true
  # TODO: remove after SW-24230 is done
  OPTIMIZE_DMA_ENGINES_ALLOCATION: 1
  # This environment variable is needed for multi-node training with Horovod.
  # Set this to be a comma-separated string of host IP addresses, e.g.:
  # MULTI_HLS_IPS: "x.x.x.x,y.y.y.y"
  # Set this to the network interface name for the ping-able IP address of the host on
  # which the training script is run. This appears in the output of ip addr.
  MPI_TPC_INCLUDE: "eno1"
  # This is the port number used for rsh from the docker container, as configured
  # in /etc/ssh/sshd_config
  DOCKER_SSHD_PORT: 3022
  # Only for bert base
  # cluster slicing optimization tested for bert base, works well with this size
  TF_PRELIMINARY_CLUSTER_SIZE: 1000

parameters:
  # Model variant, possible values: tiny, mini, small, medium, base, large
  model_variant: base
  # Training command, possible values: finetuning, pretraining
  command: finetuning
  # Test-set, possible values: mrpc, squad (finetuning), bookswiki, overfit (pretraining)
  test_set: mrpc
  data_type: bf16
  use_horovod: False
  num_workers_per_hls: 8

  dataset_parameters:
    mrpc:
      epochs: 3
      data_type_parameters:
        bf16:
          batch_size: 64
        fp32:
          batch_size: 32
      max_seq_length: 128
      learning_rate: 2e-5
      output_dir: "$HOME/tmp/mrpc_output/"
      dataset_path: "/software/data/tf/data/bert/MRPC"

    squad:
      epochs: 2
      data_type_parameters:
        bf16:
          batch_size: 24
        fp32:
          batch_size: 10
      max_seq_length: 384
      learning_rate: 3e-5
      output_dir: "$HOME/tmp/squad_base/"

    bookswiki:
      epochs: 40
      data_type_parameters:
        bf16:
          batch_size:
            - 64
            - 8
        fp32:
          batch_size:
            - 32
            - 8
      max_seq_length:
        - 128
        - 512
      iters:
        - 7038
        - 782
      warmup:
        - 2000
        - 200
      dataset_path: "/software/data/tf/data/bert/books_wiki_en_corpus/tfrecord/"
      output_dir: "$HOME/tmp/pretraining/"
      fast_perf_only: 0
      no_steps_accumulation: 0

    overfit:
      epochs: 1
      batch_size: 32
      max_seq_length: 128
      iters: 200
      warmup: 10
      dataset_path: "$HOME/tmp/bert-pretraining-overfit-dataset"
      output_dir: "$HOME/tmp/bert-pretraining-overfit-output/"
      init_checkpoint_path: "/software/data/bert_checkpoints"

